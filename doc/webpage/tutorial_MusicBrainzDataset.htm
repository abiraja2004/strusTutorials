<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 2.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<link rel="icon" type="image/ico" href="images/strus.ico" />
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="Tutorial for strus, a collection of C++ libraries for building a full-text search engine." />
	<meta name="keywords" content="fulltext search engine C++" />
	<meta name="author" content="Patrick Frey &lt;patrickpfrey (a) yahoo (dt) com&gt;" />
	<link rel="stylesheet" type="text/css" href="text-profile.css" title="Text Profile" media="all" />
	<title>Strus tutorial</title>
</head>

<body>
<script type="text/javascript">
</script>
<div id="wrap">
<div id="content">
<h1>Strus tutorial (with the MusicBrainz dataset)</h1>
<h2>Introduction</h2>
<p class="description">
This is a tutorial we show how documents can be inserted into a strus index and how they can
be queried with a command line tools. Unfortunately the part using other clients for query (like PHP)
is not finished yet.
</p>
<h2>Dataset</h2>
<p class="description">
We use the MusicBrainz dataset downloaded from <a href="https://musicbrainz.org">MusicBrainz</a>.
The dataset is licenced under the terms described <a href="https://musicbrainz.org/doc/MusicBrainz_Database/Download#License">here</a>.
Thanks to MusicBrainz to make this dataset publicly available.
The dataset contains a list of about 282'000 records with title, actor and id.
The tutorial will show, how this data can be made retrievable with strus.
</p>

<h2>Prerequisites</h2>
<p class="description">This tutorial is based on <h href="https://www.docker.com/">docker</a>.
In order to run it you need to <a href="https://docs.docker.com/installation">install docker</a> on your machine.
This tutorial has been written for Linux. For other platforms the shell commands may look different.

<h2>Go</h2>
<a name="docker"> </a>
<h3>First step: Start the strus docker container in a shell</h3>
<p class="description">
The following command runs the docker container with strus and nginx as webserver from the strus image with a terminal using bash:
<pre>
docker run -p 40080:80 -t -i patrickfrey/strus-ub1404-nginx /bin/bash
</pre>
</p>

<a name="data"> </a>
<h3>Second step: Download and prepare the data</h3>
<p class="description">
If you want to skip this section, you can simply execute the script <a href="https://raw.githubusercontent.com/patrickfrey/strusTutorials/master/scripts/musicbrainz/step1_fetchData.sh">step_fetchData.sh</a>.
This script will execute the steps described here in about a minute on a convenient developer machine.
<br/>
Otherwise you have to manually fetch the data from <a href="https://musicbrainz.org/doc/MusicBrainz_Database/Download">MusicBrainz</a>.
The dump of the record releases will be dumped and converted into a proprietary XML we will use.
Because there are many items to process, we will put up to 100 entries into one file. The analyzer
will be configured to handle one item of these 100 entries as one document.
Download and decompress the data with the following commands:
<br\>
<h4>Prepare data directory</h4>
<p class="description">We create a directory for the data because the MusicBrainz files are all unpacked in current directory.
<pre>
mkdir data
cd data
</pre>
</p>
<h4>Download and uppack the data dump</h4>
<p class="description">
<pre>
wget http://ftp.musicbrainz.org/pub/musicbrainz/data/fullexport/LATEST
LATEST=`cat LATEST`
wget http://ftp.musicbrainz.org/pub/musicbrainz/data/fullexport/$LATEST/mbdump-cdstubs.tar.bz2
bzip2 -d mbdump-cdstubs.tar.bz2
tar -xvf mbdump-cdstubs.tar
</pre>
</p>
<h4>Create the documents</h4>
<p class="description">We count the maximum document identifier that will be assigned and we 
set $ndocs as the maximum number of documents we will get. In every document will be at maximum 100
items to insert. For every document identifier we create a document with the XML header.
<pre>
cat mbdump/release_raw | sed 's/\&amp;/\&amp;amp;/g' | sed 's/&lt;/\&amp;lt;/g' | sed 's/&gt;/\&amp;gt;/g' > dump.txt
ndocs=`cat dump.txt | awk -F"\t" '{if(min=="")min=max=$1; if($1>max) {max=$1}; if($1< min) {min=$1}; } END {print int((max+1)/100)}'`
mkdir doc
idoc=0
while [ $idoc -lt $ndocs ]
do
    echo "&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\" ?&gt;" > doc/$idoc.xml
    echo "&lt;list&gt;" &gt;&gt; doc/$idoc.xml
    idoc=`expr $idoc + 1`
done
</pre>
</p>
<h4>Fill the documents with the content of the dump</h4>
<p class="description">For every entry we want to process, we create an own XML tag. Every record
is marked with an '&lt;item&gt;...&lt;/item&gt;' tag.
<pre>
cat dump.txt \
  | awk -F"\t" '{FNAME=int($1/100); print "&lt;item&gt;&lt;id&gt;" $1 "&lt;/id&gt;&lt;title&gt;" $2 "&lt;/title&gt;&lt;artist&gt;" $3 "&lt;/artist&gt;&lt;date&gt;" $4 "&lt;/date&gt;&lt;upc&gt;" $9 "&lt;/upc&gt;&lt;note&gt;" $10 "&lt;/note&gt;&lt;/item&gt;" >> "doc/"FNAME".xml" }'
idoc=0
while [ $idoc -lt $ndocs ]
do
    echo "&lt;/list&gt;" &gt;&gt; doc/$idoc.xml
    idoc=`expr $idoc + 1`
done
</pre>
</p>
<h4>Leave the data directory and inspect the result</h4>
<p class="description">
We have now preprocessed the MusicBrainz example collection with about 282'000 records.
We go back to the upper directory and inpect what we got. There should be about 3200 files
and each file should contain about 100 items.
<pre>
ls doc/
</pre>
We get:
<pre>
0.xml     1267.xml  1536.xml  1805.xml  2074.xml  2343.xml  2612.xml  2882.xml  3150.xml  461.xml  730.xml
1.xml     1268.xml  1537.xml  1806.xml  2075.xml  2344.xml  2613.xml  2883.xml  3151.xml  462.xml  731.xml
... (more)
</pre>
</p>
<p class="description">Here is an except of an example input file for strus. We can inspect the first 5 lines with:
<pre>
head -n 7 doc/999.xml 
</pre>
</p>
<p class="description">We get:
<pre>
&lt;?xml version="1.0" encoding="UTF-8" standalone="yes" ?&gt;
&lt;list&gt;
&lt;item&gt;&lt;id&gt;99929&lt;/id&gt;&lt;title&gt;Flutterstrut&lt;/title&gt;&lt;artist&gt;Hundred Year Flood&lt;/artist&gt;&lt;date&gt;2009-06-18 01:37:51.35028+00&lt;/date&gt;&lt;upc&gt;881387000018&lt;/upc&gt;&lt;note&gt;CD Baby id:hyf2&lt;/note&gt;&lt;/item&gt;
&lt;item&gt;&lt;id&gt;99999&lt;/id&gt;&lt;title&gt;The Hanging Garden&lt;/title&gt;&lt;artist&gt;Hypnotique&lt;/artist&gt;&lt;date&gt;2009-06-18 01:37:52.548532+00&lt;/date&gt;&lt;upc&gt;634479159961&lt;/upc&gt;&lt;note&gt;CD Baby id:hypnotique&lt;/note&gt;&lt;/item&gt;
&lt;item&gt;&lt;id&gt;99994&lt;/id&gt;&lt;title&gt;Hypnopilot&lt;/title&gt;&lt;artist&gt;Hypnopilot&lt;/artist&gt;&lt;date&gt;2009-06-18 01:37:52.49057+00&lt;/date&gt;&lt;upc&gt;088907200068&lt;/upc&gt;&lt;note&gt;CD Baby id:hypnopilot&lt;/note&gt;&lt;/item&gt;
&lt;item&gt;&lt;id&gt;99995&lt;/id&gt;&lt;title&gt;The Sphinx of Imagination&lt;/title&gt;&lt;artist&gt;Hypnotica&lt;/artist&gt;&lt;date&gt;2009-06-18 01:37:52.503809+00&lt;/date&gt;&lt;upc&gt;634479143427&lt;/upc&gt;&lt;note&gt;CD Baby id:hypnotica&lt;/note&gt;&lt;/item&gt;
</pre>
Every file created contains about 100 items to insert as document. The grouping of multiple items
has been done to reduce the number of files created.
</p>
<a name="create"> </a>
<h3>Third step: Create the storage</h3>
<h4>Initialize the storage database</h4>
<p class="description">Will will use the <h href="utilities.htm">utility programs of strus</a> to
create the storage and insert the documents into the search index.<br/>
We have one data element 'date' we would like to use for meta data restrictions.
The meta data used for restrictions are put into the meta data table. We can at any time alter the
meta data table or add new elements with the command <a href="utilities.htm#strusAlterMetaData">strusAlterMetaData</a>. But we can also define 
some meta data within the <a href="utilities.htm#strusCreate">strusCreate</a> command.<br/>
The command to create the storage looks as follows:
<pre>
strusCreate -s "path=storage; metadata=date UINT32"
</pre>
</p>
<p class="description">
The first argument describes the properties of the storage created with a semicolon-separated list of 
configuration assigments. The path argument of it
defines the directory the key/value store database based on leveldb will write its files to.
The storage configuration string is dependend on the storage and key/value store database we use.
All following commands introduced in this tutorial later that refer to a storage will 
have a configuration string parameter with the same syntax, but depending on the case 
with different parameters.<br/>
With this call the storage has been created in the subdirectory 'storage'. Type
</p>
<pre>
ls storage
</pre>
</p>
<p class="description">
and we will see (LevelDB files)
<pre>
000005.log  CURRENT  LOCK  LOG  LOG.old  MANIFEST-000004
</pre>
</p>
<a name="analyzer"> </a>
<h3>Fourth step: Configure the document analyzer</h3>
<p class="description">
We will use the standard document segmenter based on textwolf as we will process XML documents. 
The analyzer configuration describes, what elements will be extracted from the items and how items
are defined. It also describes how these items are transformed into the form they are stored.
The analyzer configuration defines different sections that describe what type of elements are 
created. For creating an analyzer configuration we create a file
with the extension ".ana" and start to edit it:
<pre>
touch tutorial.ana
vi tutorial.ana
</pre>
</p>
<p class="description">
The following sub sections show what we define in this configuration file. 
The complete analyzer configuration is defined in <a href="https://raw.githubusercontent.com/patrickfrey/strusTutorials/master/config/musicbrainz/tutorial.ana">tutorial.ana</a>.
The textwolf segmenter used in this tutorial defines XML selection expressions is the
abbreviated syntax of XPath with 2 exceptions:
<ol>
<li>Tag selections like "/list/item/title" do not select a subtree but just the tag.</li>
<li>Contect selections like "/list/item/title::text()" are written as "/list/item/title()" 
(the "::text" is omitted).</li>
</ol>
</p>
<h4>Define the items to index (collection units aka documents)</h4>
<p class="description">As we saw in the preparation of the documents there are about 100 items 
inserted into one file. For separating these items as documents to insert we have to define a
section document that defines what selection expression defines a document in an input file:
<pre>
[Document]
	doc = /list/item;
</pre>
</p>
<h4>Define the the search index (retrievable items)</h4>
<p class="description">
The elements that are subject of retrieval are put into the section [SearchIndex]. 
We decide to make words in the tags "title","artist" and "note" retrievable.<br/>
All of them we tokenize the same way (as words) and we use the snowball stemmer ("stem")
followed by a lowercase conversion ("lc") and a diacritical character conversion ("convdia")
to normalize them.<br/>
Our language we instrument the stemmer and the diacritical characer
conversion with is English ("en").<br/>
The feature type name we assign to the produced search index features is for all features "word".
<pre>
[SearchIndex]
	word = convdia(en):stem(en):lc  word  /list/item/title();
	word = convdia(en):stem(en):lc  word  /list/item/artist();
	word = convdia(en):stem(en):lc  word  /list/item/note();
</pre>
</p>
<p class="description">
Intuitively this configuration is understod if we read every feature assignment from
right to left: Select an element, select the tokens from an element selection, normalize the tokens: "lc -&gt; stem -&gt; convdia"
and assign them to a feature type "word".
</p>

<h4>Define the elements the forward index (for summarization)</h4>
<p class="description">
The elements that are used for summarization are put into the section [ForwardIndex].
We decide to use the retrievable words in the tags "title","artist" and "note"
also for summarization to show what query features matched in the document. 
Because the summary should show the original content of the document without markup, 
we use a whitespace tokenizer ("split") and no normalizer ("orig") to convert them for insert.
<pre>
[ForwardIndex]
	orig = orig split /list/item/title();
	orig = orig split /list/item/artist();
	orig = orig split /list/item/note();
</pre>
</p>
<p class="description">
Intuitively this configuration is understod as the search index definition by reading the
processing steps from right to left.
</p>
<h4>Define the attributes</h4>
<p class="description">
Now assign some attributes to each document inserted. Attributes are used in the representation
of the result. There is one reserved attribute name that is very important for our case. It is
the attribute "<b>docid</b>". When defining it, the strusInsert program will define the content
of this attribute as document identifier. If we do not define it, the input file path will be
used as document identifier. Because the document identifier has to be unique, we have to create
one when there are multiple documents grouped in one input file as we did before. 
For all four elements we will use a tokenization ("content") and normalization ("orig"),
that keeps the elements in their original state:
<pre>
[Attribute]
	<b>docid</b> = orig content /list/item/id();
	title = orig content /list/item/title();
	upc = orig content /list/item/upc();
	note = orig content /list/item/note();
</pre>
</p>
<p class="description">
The definitions of the section attribute is read intuitively from right to left as the
elements introduced before with the exception that the leftmost identifier specifies the
name of the attribute the value is assigned to.
</p>

<h4>Define the meta data</h4>
<p class="description">
As final step we assign the meta data element 'date' we want to use for restrictions but also
in the representation of the result. For converting the date into a format we can use as
meta data table element we use the 'date2int' normalizer. As start date of the date as integer
calculation we chose the year of the invention of the phonograph by Thomas Edison 
and we define the unit as number of days since then. The input format we deduce by looking 
at an input sample (bravely assuming here that all dates have the same format): 
<pre>
2009-06-18 01:37:52.503809+00
</pre>
</p>
<p class="description">
The tokenization is just forwarding the tag content. Here is the configuration of the date:
<pre>
[MetaData]
	date = date2int("d 1877-01-01", "%Y-%m-%d %H:%M:%s *") content /list/item/date();
</pre>
</p>
<p class="description">
The definitions of the section attribute is read intuitively from right to left as the
elements introduced before with the exception that the leftmost identifier specifies the
name of the meta data element the value is assigned to. 
</p>

<h4>Check the configuration</h4>
<p class="description">
The be sure that we got everything right, we now test what we got with one candidate input file.
We can do this by calling the program <a href="utilities.htm#strusAnalyze">strusAnalyze</a>
with our configuration and an input file:
<pre>
strusAnalyze tutorial.ana doc/2333.xml  | less
</pre>
</p>
<p class="description">
We have a look at the first document analyzed and see that all went well
(Multi part document results of the strusAnalyze program are separated by '-- document ' 
followed by the identifier assigned to it in the [Document] section):
<pre>
-- document doc

search index terms:
1 word 'live'
2 word 'in'
3 word 'milan'
5 word '27'
6 word '9'
7 word '1956'
8 word 'maria'
9 word 'calla'

forward index terms:
1 orig 'Live'
2 orig 'in'
3 orig 'Milan'
4 orig '(27,9,1956)'
8 orig 'Maria'
9 orig 'Callas'

metadata:
date '48527'

attributes:
docid '233372'
title 'Live in Milan (27,9,1956)'
</pre>
</p>
<h4>Check single steps of the analyzer configuration</h4>
<p class="description">
What do we do when something went wrong when checking a document with strusAnalyze ?<br/>
Let's assume that our output was empty. There are tools to check the different steps of
document analysis. The first program is <a href="utilities.htm#strusSegment">strusSegment</a>
that allows us to check wheter a selection expression matched. Let's take the document title
as example:
<pre>
strusSegment -e '/list/item/title()' doc/2333.xml  | head -n 2
</pre>
The output looks as follows:
<pre>
'Live in Milan (27,9,1956)'
'Greatest Hits'
</pre>
</p>
<p class="description">
This looks good as expected. The second program for checking analyzer steps is
<a href="utilities.htm#strusAnalyzePhrase">strusAnalyzePhrase</a>. This program
allows us to check tokenizers and normalizers used in the configuration.
Let's check if words are tokenized and normalized correctly:
<pre>
echo 'Live in Milan (27,9,1956)' | strusAnalyzePhrase -n "convdia(en):stem(en):lc" -t "word" -
</pre>
This leads to the following output:
<pre>
1 'live'
2 'in'
3 'milan'
4 '27'
5 '9'
6 '1956'
</pre>
</p>
<p class="description">
At the first glance this looks good. But why are the positions assigned different 
compared with the analyzer run ?
The word '27' has position 5 when passed through the analyzer with our configuration
and 4 when calling strusAnalyzePhrase. The reason is that the positions are assigned
globally to all elements of the configuration. Every by byte position in the document
defined by any tokenizer configured is respected when assigning the word positions.
The strusAnalyzePhrase does not have this information, so it assigns the positions ascending.
Because we have added in the analyzer configuration a method that produces a token '(27,9,1956)'  
that has a byte position between "Milan" and "27", this token gets the word position 4.
The token '27' gets the following position assigned, in our example 5. The reason for 
this is that the positions of all the features in the forward index and the search index
have to be in sync to define their occurrence relation by their word position.
</p>
<a name="insert"> </a>
<h3>Fifth step: Insert the documents into the storage</h3>
<p class="description">
We got now the analysis of the documents done. Now we would like to insert the documents
into the storage created. The program <a href="utilities.htm#strusInsert">strusInsert</a>
helps us to do that. It has several options to boost the insert process. It depends very 
much on the collection and the hardware used which parameters are best.<br/>
If we know that all documents inserted are new, then we should specify the option '-n' ('--new').
In this case the inserter can assume that all files inserted are new and thus it can allocate
document numbers in ranges. Otherwise the storage issues an immediate write for every
new document inserted and this slows down the insert process dramatically because
we have only new documents now.<br/>
Another important option is '-c' ('--commit') that sets the number of documents inserted
within one transaction. Because out documents are very small (less than a dozen terms),
the default of 1000 is not very reasonable. Better is 50000.<br/>
Here is our insert command with the proposed options for this collection:
<pre>
strusInsert -n -c 50000 -f 100 -s "path=storage" tutorial.ana doc
</pre>
</p>
<p class="description">
that does not use multiple inserter threads (default), commits an open transaction 
after 50000 documents inserted, preallocates document numbers in ranges and fills the 
storage in the folder 'storage' with XML documents analyzed with the configuration tutorial.ana.
This should output the following 2 lines after roughly a minute or half a minute:
<pre>
inserted 282291 documents (total 282291), 0 transactions open     
done
</pre>
</p>

<a name="inspect"> </a>
<h3>Sixth step: Inspect the storage</h3>
<p class="description">
The following examples use the dump from 29th of June 2015. We might get different
results with another dump.<br/>
</p>
<p class="description">
Now as we have inserted all documents we are curious to see
if everything went right. There is a program
<a href="utilities.htm#strusDumpStorage">strusInspect</a> that lets us inspect
some properties of the storage. We chose the document with the document id
'10009'. First we find out what internal document number this docid got:
<pre>
strusInspect -s "path=storage" docno 10009
</pre>
we get
<pre>
123
</pre>
</p>
<p class="description">
Then we do some calls to inspect this document
<pre>
strusInspect -s "path=storage" attribute docid 123

strusInspect -s "path=storage" content orig 123
strusInspect -s "path=storage" attribute title 123

strusInspect -s "path=storage" pos word danc 123
strusInspect -s "path=storage" ff word danc 123
strusInspect -s "path=storage" df word danc 

strusInspect -s "path=storage" metatable
strusInspect -s "path=storage" metadata date 123
</pre>
</p>
<p class="description">
and we get
<pre>
10009

Absolute Let's dance 13 Various
Absolute Let's dance 13

4
1
1575

date UInt32
48253
</pre>
</p>
<p class="description">
If we want to see more at once we can also use the dumpers 
<a href="utilities.htm#strusDumpStorage">strusDumpStorage</a> and 
<a href="utilities.htm#strusDumpStatistics">strusDumpStatistics</a> to get more
info. But for understanding the storage dump there is knowledge about the internal
block structure required. The programs <a href="utilities.htm#strusCheckStorage">strusCheckStorage</a>
and <a href="utilities.htm#strusCheckInsert">strusCheckInsert</a> are also useful for performing
some checks on the storage created.
</p>
<p class="description">
It is important to know that the dump program <a href="utilities.htm#strusDumpStorage">strusDumpStorage</a>
does not output any data format that we can rely on or that we can import again. This dump is not
for backup or for further processing.
</p>

<a name="cmdquery"> </a>
<h3>Seventh step: Example query by command line</h3>
<p class="description">
In this section we examine the possibility to issue queries by the command line tools.
The command line tool for query has 3 different languages defined.
One for <a href="grammar_analyzerprg_qry.htm">analyzing query phrases</a> that is similar to the analyzer configuration, one for
defining the <a href="grammar_queryevalprg.htm">query evaluation scheme</a>
and one for the <a href="grammar_query.htm">query</a>. 
This is quite a lot for a tutorial. We can alterntively skip this section and go to the 
section explaining <a href="#phpquery">how to issue a query with PHP</a>. The mechanisms there do not
involve any of the languages indroduced here.
<br/>
For issuing a query by command line we use the program <a href="utilities.htm#strusQuery">strusQuery</a>.
</p>
<h5>Query phrase analyzer configuration</h5>
<p class="description">
First we need to create an analyzer configuration for query phrases. We name the file <u>query.ana</u>:
<pre>
word = convdia(en):stem(en):lc word;
</pre>
</p>
<p class="description">
It looks like an excerpt of the document analyzer configuration we wrote before.
And this similarity is wished because we intent to normalize and name the query features
in the same way as the document features, so that they match each other. As in the
document analyzer configuration the lines are intuitively understood by reading them
from right to left: Tokenize by selecting the words and normalize it by applying lower case,
stemming with snowball and conversion of diacritical characters to get elements of the
feature type word.
</p>
<h5>Query evaluation scheme configuration</h5>
<p class="description">
In a second step we define a simple query evaluation scheme. We name the file of the
query evaluation scheme <u>query.eva</u>:
<pre>
SELECT qry;
EVAL tf( .match=qry );

SUMMARIZE docid = attribute( name=docid );
SUMMARIZE title = attribute( name=title );
</pre>
</p>
<p class="description">
The language has only 4 commands. TERM to define structural terms that are not defined in
the query, but that are subject to query evaluation like for example sentence delimiters.
SELECT to define the set of query features that define what documents are weighted.
EVAL to define a query evaluation scheme to use. SUMMARIZE to define a summarization
step that is done to accumulate attributes of the documents presented as result.<br/>
One thing that might puzzle is the name 'qry' for the feature sets. Why not 'word'
as we indexed the terms in the query and in the document. Strus distinguishes between
term types, that classify a term and the feature set, that defines a name to address
a group of features. In this tutorial where we have just a simple document struture
and one list of query terms, this distinction might not be justified. But in more
complex cases where we have expressions involved, many levels of evaluation, a selection
of features that is different to the features weighted and a
summarization that works on different sets of features than weighting and selection,
we need another level on which elements of the query are grouped and addressed with.<br/>
This query evaluation scheme is read as follows:
<br/>
Take the documents that contain any matches of the features of the set 'qry' and evaluate
the simple scheme 'tf' (count query features matching ~ feature weight * ff).
Show the document id and the title attribute of the document in the result.
</p>
<h5>Query source file</h5>
<p class="description">As third element we need a query. We name the file with the query
<u>query.qln</u>:
<pre>
[Feature qry :word]
  dance
</pre>
</p>
<p class="description">
A query has two type of sections [Feature] and [Condition].
The later one that defines restriction expressions on meta data is not used in our simple
example query. The first one [Feature qry:word] defines an expression and assings it to
the feature set 'qry', that is the feature set we referenced in query evaluation.
The default phrase type used for analyze phrases is 'word'. This selects the method to be used to
analyze a phrase that has not explicit specification of the phrase type. This is the
case in our query that just contains one term 'dance'.
</p>
<h5>Issue the query on command line</h5>
<p class="description">For issuing our query, we call:
<pre>
strusQuery -s "path=storage" query.ana query.eva query.qln
</pre>
</p>
<p class="description">and we get our first search result with strus:</p>
<pre>
ranked list (starting with rank 0, maximum 10 results):
[1] 25357 score 3
        docid (124428)
        title (Dance Dance Dance)
[2] 36620 score 3
        docid (135366)
        title (Dance Dance Dance)
[3] 199006 score 3
        docid (313932)
        title (Dance - Dance - Dance)
[4] 172756 score 3
        docid (283564)
        title (Time for Dancing Ballroom Dance & Latin Ballroom Dance, Volume 4)
[5] 255422 score 3
        docid (73977)
        title (Dance, Dance, Dance)
[6] 203579 score 3
        docid (318916)
        title (Dance!)
[7] 199597 score 3
        docid (314534)
        title (Good Friends, a fifth selection of dances from Calculated Figures)
[8] 152754 score 3
        docid (259327)
        title (Russian Folk Dances of the Moiseyev Dance Company)
[9] 2630 score 2
        docid (102456)
        title (Israel Dances - Greatest Folk Dances)
[10] 280052 score 2
        docid (97811)
        title (Hannah & Mariah Dancing)
</pre>
</p>
<a name="phpquery"> </a>
<h3>Eighth step: Example query with PHP</h3>
<p class="description">
In this section we examine the possibility to issue queries in a scripting language.
As example we use the PHP bindings to query our strus storage built.
The PHP scripts running in the webserver context can not directly access the storage
files. The strus PHP commands issued are using a proxy implementation of the strus
interface that redirect the calls to be executed on a server. This server is implemented
as program <a href="utilities.htm#strusRpcServer">strusRpcServer</a>. 
Because we started the image with a shell, the docker CMD starting the strus server
was ignored. So we have to start it by hand in the background:
<pre>
strusRpcServer -s "path=storage" &
</pre>
</p>
<h4>Test execution chain</h4>
<p class="description">
We start with a simple PHP script that asks the strus storage for the number of documents
inserted. We call the script <u>status.php</u>:
<pre>
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;body&gt;
&lt;?php
require "strus.php";
try {
	// Create the strus context as proxy redirecting the 
	//	commands to the strus server listening on port 7181:
	$context = new StrusContext( "localhost:7181" );
	// Create the storage client:
	$storage = $context-&gt;createStorageClient( "" );

	echo '&lt;p&gt;';
	// Print the number of documents inserted:
	echo "Number of documents inserted: " . $storage-&gt;nofDocumentsInserted() . "!";
	echo '&lt;/p&gt;';
}
catch ( Exception $e) {
	// Handle possible error:
	echo '&lt;p&gt;&lt;font color="red"&gt;Caught exception: ' . "{$e-&gt;getMessage()}&lt;/font&gt;&lt;/p&gt;";
}
?&gt;
&lt;/body&gt;
&lt;/html&gt; 
</pre>
</p>
<p class="description">
In order to make it accessible for nginx, we have to copy it to the web servers html pages 
root directory and give it the right permissions:
<pre>
cp status.php /usr/share/nginx/html/
chown www-data:www-data /usr/share/nginx/html/status.php
chmod +x /usr/share/nginx/html/status.php
</pre>
</p>
<p class="description">
Now we can call it via HTTP, say we can enter the following URL into a web browsers address bar.
We have specified port 40080 when starting the strus docker container, so we also specify it here:
</p>
<pre>
http://127.0.0.1:40080/status.php
</pre>
</p>
<p class="description">
and we get:
</p>
<img src="numberOfDocuments.jpg" alt="Number of documents" height="200"> 
<p class="description">
</p>
<h4>Issue a query</h4>
<p class="description">Now we try to create a simple page showing a ranked list of best matches
to a query passed as GET parameters. We call the PHP source file <u>search.php</u>:
<pre>
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;body&gt;
&lt;?php
require "strus.php";
<font color=green>
// The following helper function will be used to evaluate a query:</font>
function evalQuery( $queryString, $minRank, $maxNofRanks)
{	<font color=green>
	// We create the strus context as proxy to the strusRpcServer
	// started in our docker container listening on port 7181:</font>
	$context = new StrusContext( "localhost:7181" );
	<font color=green>
	// We create the storage proxy interface refering to the storage 
	// served by the server by passing an empty storage configuration:</font>
	$storage = $context-&gt;createStorageClient( "" );
	<font color=green>
	// We create the analyzer proxy interface:</font>
	$analyzer = $context-&gt;createQueryAnalyzer();
	<font color=green>
	// We create the query evaluation proxy interface:</font>
	$queryeval = $context-&gt;createQueryEval();
	<font color=green>
	// Here we define how the query is analyzed. In our example we will introduce
	// only one phrase type and call it "qry". The produced feature type "word" matches
	// to the feature type in the search index of the document. The tokenizer used
	// is the same as for the features in the document ("word"). So is the normalizer
	// defined. Note that the execution of the normalizer is read from left to
	// right and not as in the analyzer configuration language from right to left.</font>
	$analyzer-&gt;definePhraseType( "qry", "word"/*feat type*/, "word"/*tokenizer*/, 
			["lc", ["stem", "en"], ["convdia", "en"] ] /*normalizer*/);
	<font color=green>
	// Now we define our weighting function, that counts the number of query
	// feature matches in the document. Weighting function attribute names,
	// that reference features, start with a dot '.'. Others referencing
	// number or string constants not:</font>
	$queryeval-&gt;addWeightingFunction( 1.0, "tf", [".match" =&gt; "qry" ]);
	<font color=green>
	// Next we define what is presented as attribute of the result.
	// We would like to see the document identifier and the document title:</font>
	$queryeval-&gt;addSummarizer( "DOCID", "attribute", [ "name" =&gt; "docid" ] );
	$queryeval-&gt;addSummarizer( "TITLE", "attribute", [ "name" =&gt; "title" ] );
	<font color=green>
	// For the documents to match we have to define a selection expression.
	// The selected features will be addressed by the feature set identifier "sel":</font>
	$queryeval-&gt;addSelectionFeature( "sel");
	<font color=green>
	// Now we have finished the query evaluation scheme definition and we create
	// the query to issue. First we define the query object to instantiate:</font>
	$query = $queryeval-&gt;createQuery( $storage);
	<font color=green>
	// Next we take the query string as phrase and analyze it. In a more complex
	// system we would parse the query language first and the elements of the
	// query language would be the queryphrases to analyze. In our simple example
	// setting, we have just a single phrase query:</font>
	$terms = $analyzer-&gt;analyzePhrase( "qry", $queryString);

	if (count( $terms) &gt; 0)
	{	<font color=green>
		// Now we create the query expressions. The selection feature "sel" will
		// be represented as selection of documents that contain all query features.
		// The weighted features "qry" will be the set of terms in the query.
		// The query is constructed with operations on a stack:</font>
		foreach ($terms as &$term)
		{
			$query-&gt;pushTerm( "word", $term-&gt;value);
			<font color=green>// ... The term pushed is now the topmost element on the stack
			</font>
			$query-&gt;pushDuplicate();<font color=green>
			// ... We push another instance of the term
			</font>
			$query-&gt;defineFeature( "qry");<font color=green>
			// ... One we take from the stack and create a feature 
			// the other copy remains on the stack and will be used to create
			// the selection expression ("sel").</font>
		}
		$query-&gt;pushExpression( "contains", count($terms), 0);<font color=green>
		// ... all copied features (pushDuplicate) are taken from the stack and an
		// expression is constructed that matches all documents that
		// contain all the features. The expression is pushed on the stack.
		</font>
		$query-&gt;defineFeature( "sel");<font color=green>
		// ... Define the selection feature "sel" from the expression on the top of the stack.</font>
	}
	<font color=green>
	// At last we define the number of documents to retrieve and the first rank to show:</font>
	$query-&gt;setMaxNofRanks( $maxNofRanks);
	$query-&gt;setMinRank( $minRank);
	<font color=green>
	// And we evaluate the query and return it as result:</font>
	return $query-&gt;evaluate();
}

<font color=green>
// Parse the query parameters:</font>
parse_str( getenv('QUERY_STRING'), $_GET);
$queryString = $_GET['q'];
$nofRanks = 20;
$minRank = 0;
if (array_key_exists( 'n', $_GET))
{
	$nofRanks = intval( $_GET['n']);
}
if (array_key_exists( 'i', $_GET))
{
	$minRank = intval( $_GET['i']);
}
<font color=green>
// Call the defined subroutine to evaluate the query:</font>
$results = evalQuery( $queryString, $minRank, $nofRanks);
<font color=green>
// Display the result:</font>
echo '&lt;table border=1&gt;';
foreach ($results as &$result)
{
	echo '&lt;tr&gt;&lt;td&gt;' . $result-&gt;DOCID . '&lt;/td&gt;&lt;td&gt;' . $result-&gt;TITLE . '&lt;/td&gt;&lt;/tr&gt;' . "\n";
}
echo '&lt;/table&gt;';
?&gt;
&lt;/body&gt;
&lt;/html&gt; 
</pre>
</p>
<p class="description">
Now we copy the file into the webservers document area and set the permissions correctly:
<pre>
cp search.php /usr/share/nginx/html/
chown www-data:www-data /usr/share/nginx/html/search.php
chmod +x /usr/share/nginx/html/search.php
</pre>
</p>
<p class="description">
Now we can call the search page via HTTP, say we can enter the following URL into a web browsers address bar
as we did it with the <u>status.php</u> page. We search for "party dance", and retrieve the
first 10 ranks starting with the best match:
<pre>
http://127.0.0.1:40080/search.php?q=party%20dance&amp;i=0&amp;n=10
</pre>
<p class="description">
and we get:
</p>
<img src="searchResult.jpg" alt="Search Result" height="400"> 
</p>

<h3>Ninth step: Think about build your own search application</h3>
<p class="description">
This tutorial hopefully showed that it is simple to create a search engine for a simple data set.
Unfortunately search applications are normally not simple at all. But if you have ideas or
questions about the strus search engine, please contact us by mail.
</p>
</div>
</div>
</body>
</html>

